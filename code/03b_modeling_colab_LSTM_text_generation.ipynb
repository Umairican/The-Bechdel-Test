{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cooked-policy",
   "metadata": {
    "id": "unusual-daily"
   },
   "source": [
    "# The Below Notebook is for a text generation model with LSTM.\n",
    "\n",
    "I recommend running this notebook in Google Colab with a GPU.\n",
    "\n",
    "\n",
    "### The alternative is to bring in movies that are mainly women and use those scripts:\n",
    "\n",
    "- Bridesmaids\n",
    "- Ghost World\n",
    "- Juno\n",
    "- Martha Marcy May Marlene\n",
    "- Precious\n",
    "- Sex and the City\n",
    "- The Help\n",
    "- Frozen\n",
    "\n",
    "I believe that, at around 20,000 words apiece, we are looking at a big enough corpus to train some sort of text generation model for screenplays.\n",
    "\n",
    "The result wasn't spectacular, but was worth the effort.\n",
    "\n",
    "\n",
    "Many thanks to Jason Brownlee, whose [article](https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/) provided much of the basis for what you see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "liable-tolerance",
   "metadata": {
    "id": "hidden-resolution"
   },
   "outputs": [],
   "source": [
    "# Bring some mates\n",
    "\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "municipal-solid",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T9tLWYDBKn-l",
    "outputId": "0388b52e-e248-4bdb-96e2-eaec343ddc13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerical-hundred",
   "metadata": {
    "id": "clinical-toronto"
   },
   "source": [
    "### We must create a dataset of the screenplays that we want to use for training.\n",
    "\n",
    "The screenplays that we are going to use is above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "applicable-nickname",
   "metadata": {
    "id": "expected-chuck"
   },
   "outputs": [],
   "source": [
    "#Lets save each script to be used as its own variable here\n",
    "bridesmaids = \"../data/Scripts/BRIDESMAIDS.TXT\"\n",
    "ghost_world = \"../data/Scripts/GHOST WORLD.TXT\"\n",
    "juno = \"../data/Scripts/JUNO.TXT\"\n",
    "martha_marcy = \"../data/Scripts/MARTHA MARCY MAY MARLENE.TXT\"\n",
    "precious = \"../data/Scripts/PRECIOUS.TXT\"\n",
    "sex_city = \"../data/Scripts/SEX AND THE CITY: THE MOVIE.TXT\"\n",
    "the_help = \"../data/Scripts/THE HELP.TXT\"\n",
    "frozen = \"../data/Scripts/FROZEN.TXT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "valid-transport",
   "metadata": {
    "id": "handy-headquarters"
   },
   "outputs": [],
   "source": [
    "raw_text = open(bridesmaids, 'r', encoding='utf-8').read() + open(ghost_world, 'r', encoding='utf-8').read() + open(juno, 'r', encoding='utf-8').read() + open(martha_marcy, 'r', encoding='utf-8').read() + open(precious, 'r', encoding='utf-8').read() + open(sex_city, 'r', encoding='utf-8').read() + open(the_help, 'r', encoding='utf-8').read() + open(frozen, 'r', encoding='utf-8').read()\n",
    "raw_text = raw_text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historical-astrology",
   "metadata": {},
   "source": [
    "### Char2Vec or Word2Vec\n",
    "\n",
    "I ran both models in my exploration, and found that, for generating a screenplay, I needed to choose Char2Vec as the model recognized the need for including the white space that is in a screenplay.\n",
    "\n",
    "Word2Vec would, however, create more legible text. But it did not have any of the screenplay structure, which was to be expected.\n",
    "\n",
    "Uncomment the tokens cells below if you would like to try this with Word2Vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "worse-financing",
   "metadata": {
    "id": "G9-ZgDlNKFaj"
   },
   "outputs": [],
   "source": [
    "#tokens = re.findall(r\"\\w+\", raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "welsh-mathematics",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokens = [word for word in tokens if not word.startswith('0')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "primary-potter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mapping of unique words to integers\n",
    "# Change this to tokens if you want word2Vec\n",
    "chars = sorted(list(set(raw_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "graphic-polyester",
   "metadata": {
    "id": "prescription-adaptation"
   },
   "outputs": [],
   "source": [
    "# create mapping of unique words to integers\n",
    "# Change this to raw_text if you just want characters\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preliminary-discipline",
   "metadata": {},
   "source": [
    "Use the below for word tokenization. Above is for characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "alike-signal",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aggressive-metro",
    "outputId": "0a9875e0-bd3c-46f5-b16e-732ccf982ab1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  1735806\n",
      "Unique Characters:  60\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print (\"Total Characters: \", n_chars)\n",
    "print (\"Unique Characters: \", n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "divided-render",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "organic-cooling",
    "outputId": "b2789681-e2af-426d-c80a-d3a2df790351"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  1735706\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "# Change raw_text to tokens for word2Vec\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "\tseq_in = raw_text[i:i + seq_length]\n",
    "\tseq_out = raw_text[i + seq_length]\n",
    "\tdataX.append([char_to_int[char] for char in seq_in])\n",
    "\tdataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print (\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "linear-project",
   "metadata": {
    "id": "appropriate-penny"
   },
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "variable-premises",
   "metadata": {
    "id": "tribal-range"
   },
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(12278, 100, input_length = seq_length))\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Extra layer, because computers need to work\n",
    "#model.add(LSTM(256))\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "developmental-airfare",
   "metadata": {
    "id": "neither-tucson"
   },
   "outputs": [],
   "source": [
    "# define the checkpoint - keep the best one\n",
    "filepath=\"char-textgen-weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfied-absolute",
   "metadata": {
    "id": "disturbed-riverside"
   },
   "source": [
    "### Fit and run!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-friday",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "offshore-template",
    "outputId": "4a8bd2bf-9047-4d2f-853a-491304f9e6bd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.fit(X, y, epochs=5, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bigger-airplane",
   "metadata": {},
   "source": [
    "We are using the below to save the model and move it over to Streamlit for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiovascular-justice",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.models.save_model(model, \"saved_final_model.hp5\", save_format=\"h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excessive-participant",
   "metadata": {},
   "source": [
    "The below is for loading the weights and then generating text from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "blind-essence",
   "metadata": {
    "id": "gn3BCHrLVrZ4"
   },
   "outputs": [],
   "source": [
    "# load the network weights\n",
    "best_model = \"../code/word-textgen-weights-improvement-20-6.8268.hdf5\"\n",
    "model.load_weights(best_model)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "primary-interview",
   "metadata": {
    "id": "5SAnmD_oV2mL"
   },
   "outputs": [],
   "source": [
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reliable-syracuse",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xuLUs3zAV3de",
    "outputId": "c0a793af-89b9-4a16-ccd0-12327510ec14"
   },
   "outputs": [],
   "source": [
    "# pick a random seed\n",
    "start = np.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print (\"FADE IN:\")\n",
    "# include a space in the join statement if doing word2Vec\n",
    "print (\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "\n",
    "# generate characters\n",
    "for i in range(50):\n",
    "\tx = np.reshape(pattern, (1, len(pattern), 1))\n",
    "\tx = x / float(n_vocab)\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = np.argmax(prediction)\n",
    "\tresult = int_to_char[index]\n",
    "\tseq_in = [int_to_char[value] for value in pattern]\n",
    "\tsys.stdout.write(result)\n",
    "\tpattern.append(index)\n",
    "\tpattern = pattern[1:len(pattern)]\n",
    "print (\"\\nFADE OUT.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noticed-intro",
   "metadata": {
    "id": "ZhOfgWCEWx3t"
   },
   "source": [
    "### Evaluations\n",
    "\n",
    "The first running of the text generation model involved some hilarious outcomes, namely the fact that, after a while, the only word being produced was, \"to\". I can't say it's a roaring success, but it has been fun to try! I deployed a char2Vec model on Streamlit, which you can also find in this repo.\n",
    "\n",
    "The word2Vec model began spewing either the word \"the\" or \"thicker\" repeatedly at the end, which is not useful. It seems to only work for around 50 words before it becomes unintelligible. And it also doesn't keep the general structure of a screenplay, which is an issue in itself."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "03-modeling-LSTM-text-generation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
